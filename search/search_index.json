{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>A high-performance, asynchronous, ready for production job scheduling framework.</p> <p>TaskShed provides a simple API to schedule your Python coroutines for later execution. You can run tasks just once or on a recurring interval. The scheduler is dynamic, allowing you to add, update or remove tasks on the fly. Furthermore, by connecting to a persistent datastore, TaskShed ensures your tasks survive restarts and automatically catches up on any executions that were missed while the system was offline.</p> <p>The key features are:</p> <ul> <li>Fast: TaskShed has an extremely low latency, overhead and can execute several thousands tasks a second.</li> <li>Distributed: TaskShed has the capacity to spawn several workers and schedules across many machines, while also providing support for monolinth architectures.</li> <li>Persistant: Tasks are stored in database, meaning that tasks won't get dropped on shutdown. TaskShed currently supports Redis and MySQL.</li> <li>Easy: TaskShed is straightforward to run. </li> </ul>"},{"location":"#installation","title":"Installation","text":"Basic Installation<pre><code>pip install taskshed\n</code></pre> <p>The base version of TaskShed has no additional dependencies, though needs a driver for task persistance. This can be installed by running:</p> RedisMySQL <pre><code>pip install \"taskshed[redis]\"\n</code></pre> <pre><code>pip install \"taskshed[mysql]\"\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Create a file <code>main.py</code> with:</p> <pre><code>from datetime import datetime, timedelta\nfrom taskshed.datastores import InMemoryDataStore\nfrom taskshed.schedulers import AsyncScheduler\nfrom taskshed.workers import EventDrivenWorker\n\n\nasync def say_hello(name: str):\n    print(f\"Hello, {name}!\")\n\n\ndatastore = InMemoryDataStore()\nworker = EventDrivenWorker(callback_map={\"say_hello\": say_hello}, datastore=datastore)\nscheduler = AsyncScheduler(datastore=datastore, worker=worker)\n\n\nasync def main():\n    await scheduler.start()\n    await worker.start()\n    await scheduler.add_task(\n        callback=\"say_hello\",\n        run_at=datetime.now() + timedelta(seconds=3),\n        kwargs={\"name\": \"World\"},\n    )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    loop = asyncio.new_event_loop()\n    loop.create_task(main())\n    loop.run_forever()\n</code></pre> <p>Then run it with.</p> <pre><code>python main.py\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>TaskShed was benchmarked against two established asynchronous and persistent task scheduling libraries - <code>APScheduler</code> and <code>ARQ</code> to assess performance in selected areas.</p> <p>The benchmarks were run on a MacBook Air (M1, 2020) with macOS Sonoma. They have not been independently verified. The primary aim was to provide a transparent, reproducible comparison of <code>TaskShed</code>\u2019s performance under specific test conditions.</p> <p>All code for these benchmarks is available in the tests/benchmarks directory of the <code>TaskShed</code> repository.</p>"},{"location":"benchmarks/#time-to-schedule-500-tasks-s","title":"Time to Schedule 500 Tasks (s)","text":"<p>This test measures how long each framework takes to schedule 500 tasks. Faster scheduling is particularly relevant for applications that dynamically create large numbers of tasks.</p> <p>In these tests, <code>TaskShed</code> scheduled tasks in less time than <code>APScheduler</code> and <code>ARQ</code>. The batched scheduling features of <code>TaskShed</code> (using either Redis or MySQL) showed especially low scheduling times.</p> <p></p>"},{"location":"benchmarks/#total-cpu-time-s-to-process-50000-tasks","title":"Total CPU Time (s) to Process 50,000 Tasks","text":"<p>This benchmark measures the total CPU time required to process 50,000 tasks. Lower CPU time can indicate higher resource efficiency, which may be beneficial in terms of operational cost and energy use.</p> <p>In these measurements, <code>TaskShed</code> used less CPU time than the other tested frameworks.</p> <p></p>"},{"location":"benchmarks/#task-execution-lag-ms","title":"Task Execution Lag (ms)","text":"<p>Execution lag refers to the time difference between a task's scheduled execution and its actual start time. Lower lag indicates that tasks are executed closer to their intended schedule.</p> <p>Again, <code>TaskShed</code> recorded lower execution lag than <code>APScheduler</code> and <code>ARQ</code>.</p> <p></p>"},{"location":"datastores/","title":"Datastores","text":"<p>Datastores are the single source of truth for scheduled tasks. TaskShed currently ships with three datastore backends:</p> <ul> <li>In Memory: Keeps tasks in Python data structures. It's fast and simple; useful for prototyping and unit tests, but does not have persistence, and probably shouldn't be used in production.</li> <li>MySQL: Uses the awesome aiomysql library to create a connection pool to a MySQL server, and asynchronously executes commands.</li> <li>Redis: Uses the equally awesome redis-py interface to the Redis key-value store using a Sorted Set for scheduling, Hashes for task payloads and Sets for groups.</li> </ul>"},{"location":"datastores/#install","title":"Install","text":"<p>In order to levearage a persistant datastore you'll need to install the additional dependencies, which can be done with:</p> RedisMySQL <pre><code>pip install \"taskshed[redis]\"\n</code></pre> <pre><code>pip install \"taskshed[mysql]\"\n</code></pre>"},{"location":"datastores/#serialization","title":"Serialization","text":"<p>Serialization is the process of converting data into a format that can be stored and reconstructed later. TaskShed serializes task payloads (the <code>kwargs</code> field) as JSON, which is human-readable and widely supported.</p> <p>That being said, there are a few downsides that you should be aware of. JSON only supports a limited set of primitive data types, such as strings, numbers, booleans, arrays, null, objects and nested combinations of these types. </p> <p>There may be occasions when you might want to store things like <code>datetime</code> objects or <code>Pydantic</code> models. As such your code will have to do additional work to convert these objects into a JSON-compatible format, i.e. calling <code>datetime.isoformat()</code> or <code>BaseModel.model_dump()</code>, and then parsed back calling <code>datetime.fromisoformat()</code> or <code>BaseModel.model_validate_json()</code>.</p> <p>The example below demonstrates this pattern with a <code>datetime</code> object:</p> Example When Passing Datetime Objects as Callback Kwargs<pre><code>from datetime import datetime, timedelta\n\nfrom taskshed.datastores import InMemoryDataStore\nfrom taskshed.schedulers import AsyncScheduler\nfrom taskshed.workers import EventDrivenWorker\n\n\nasync def calculate_task_latency(run_at: str):\n    current_time = datetime.now()\n    scheduled_time = datetime.fromisoformat(run_at)\n    latency = current_time - scheduled_time\n    print(\n        f\"\\nExecuted at:\\t{current_time}\\n\"\n        f\"Scheduled for:\\t{scheduled_time}\\n\"\n        f\"Latency:\\t{latency.total_seconds()} s\"\n    )\n\n\ndatastore = InMemoryDataStore()\nworker = EventDrivenWorker(\n    callback_map={\"calculate_task_latency\": calculate_task_latency},\n    datastore=datastore,\n)\nscheduler = AsyncScheduler(datastore=datastore, worker=worker)\n\n\nasync def main():\n    await scheduler.start()\n    await worker.start()\n\n    run_at = datetime.now() + timedelta(seconds=1)\n\n    await scheduler.add_task(\n        callback=\"calculate_task_latency\",\n        run_at=run_at,\n        kwargs={\"run_at\": run_at.isoformat()},\n    )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    loop = asyncio.new_event_loop()\n    loop.create_task(main())\n    loop.run_forever()\n</code></pre>"},{"location":"key-concepts/","title":"Key Conecpts","text":"<p>There are three central and configurable components of TaskShed:</p> <ul> <li>Scheduler: Acts as an API for developers to perform CRUD operations on tasks. It additionally has the responsibility of informing other components of changes (for instance if a new task has been submitted).</li> <li>Datastore: A data access object that stores the tasks. This could be purely in-memory datastores or a persistant storage such as <code>Redis</code> or <code>SQL</code>.</li> <li>Worker: The component responsible for executing the tasks. It pulls due tasks from the datastore and submits them to the event loop.</li> </ul> <pre><code>graph LR\n  A[Scheduler] ---&gt; B{Datastore};\n  B ---&gt; C[Worker];\n  A -...-&gt; |Notifies| C</code></pre>"},{"location":"schedulers/","title":"Schedulers","text":"<p>The only available scheduler at present is the <code>AsyncScheduler</code>. This works with a running event loop and all it's methods are coroutines.</p> <p>It requires a datastore and optionally accepts an <code>EventDrivenWorker</code>.</p> AsyncScheduler<pre><code>from taskshed.datastores import RedisDataStore\nfrom taskshed.schedulers import AsyncScheduler\nfrom taskshed.workers import EventDrivenWorker\n\n\nasync def add(a, b):\n    return a + b\n\n\ndatastore = RedisDataStore()\nworker = EventDrivenWorker(callback_map={\"add\": add}, datastore=datastore)\nscheduler = AsyncScheduler(datastore=datastore, worker=worker)\n\n\nasync def main():\n    await scheduler.start()\n    await worker.start()\n</code></pre> <p>We also need to call the <code>start()</code> method before we can start scheduling tasks.</p>"},{"location":"schedulers/#scheduling-tasks","title":"Scheduling Tasks","text":"<p>To create a task you simply call the <code>add_task()</code> method, passing in the <code>callback</code> string, a <code>run_at</code> datetime and optional keyword arguments to be passed to the target function.</p> One-off Task<pre><code>await scheduler.add_task(\n    callback=\"add\",\n    run_at=datetime.now() + timedelta(seconds=5),\n    kwargs={\"a\": 1, \"b\": 2},\n)\n</code></pre> String Callbacks <p>Rather than accepting the callback function directly, a Task take in a string that points to the eventual callback coroutine defined in the Worker <code>callback_map</code>. While slightly more cumborsome than passing in a function directly, there are two good reasons for this design:</p> <ol> <li>Distributed Architecture: The Scheduler and Worker can operate completely independently of one another. This means that you could have a Scheduler exist on one machine, and potentially many distribted Worker(s) on any number of other machines.</li> <li>Serialization: Tasks are stored as JSON in the datastores, this is considerably simpler and faster than using Python serialization libraries such as <code>pickle</code> - which is volatile to code changes.</li> </ol> <p>You can also create recurring tasks by specifying the <code>run_type</code> and passing in an <code>interval</code>:</p> Recurring Tasks<pre><code>await scheduler.add_task(\n    callback=\"add\",\n    run_at=datetime.now() + timedelta(seconds=5),\n    kwargs={\"a\": 1, \"b\": 2},\n    run_type=\"recurring\",\n    interval=timedelta(seconds=10),\n)\n</code></pre> <p>In the above example, the <code>add()</code> coroutine is scheduled to run in 5 seconds, and then once every 10 seconds.</p> <p>You can also efficiently schedule multiple tasks at once using the <code>add_tasks()</code> method. Unlike the <code>add_task()</code> method, this method accepts an iterable of <code>Task</code> objects.</p> Scheduling Many Tasks<pre><code>from taskshed.models import Task\n\n\ntasks = [\n    Task(\n        callback=\"add\",\n        run_at=datetime.now() + timedelta(seconds=i * 10),\n        kwargs={\"a\": 1, \"b\": 2},\n        task_id=f\"task-{i}\",\n        group_id=\"adding_numbers\",\n    )\n    for i in range(5)\n]\n\nawait scheduler.add_tasks(tasks)\n</code></pre> <p>This will schedule all tasks in a single trasaction and therefore is almost always more efficient than calling <code>add_task()</code> in a loop.</p> <p>Additionally, the above example also demonstrates two optional ways of identifying tasks: <code>task_id</code> and <code>group_id</code>.</p> <ul> <li>The <code>task_id</code> must be unique and can be used to fetch, modify and delete a single task. If left blank an ID will be generated.</li> <li>The <code>group_id</code> does not have to be unique and is used to perform operations on a group of tasks.</li> </ul> <p>Timezones</p> <p>TaskShed accepts <code>run_at</code> datetime values that have an attached timezone. In order to unify execution, it iternally changes this to UTC, but the Task will still be run at the time specified. For example:</p> <pre><code>from zoneinfo import ZoneInfo\n\nawait scheduler.add_task(\n    callback=\"add\",\n    run_at=datetime(year=2025, month=3, day=14, hour=10, tzinfo=ZoneInfo(\"America/New_York\")),\n)\n</code></pre> <p>Will be run on 14th of March at 10 AM, New York time (aka 2 PM UTC).</p>"},{"location":"schedulers/#fetching-pausing-resuming-and-deleting-tasks","title":"Fetching, Pausing, Resuming and Deleting Tasks","text":"<p>The API for fetching, pausing, resuming and deleting tasks are the same. Each method accepts either an iterable (list, tuple, set, etc.) of <code>task_id</code> or a single <code>group_id</code>. For example, to fetch all our adding_number tasks we simply call:</p> Fetching Tasks<pre><code>add_tasks = await scheduler.fetch_tasks(group_id=\"adding_numbers\")\n</code></pre> <p>We can also pause tasks by calling the <code>pause_tasks()</code> method. This will suspend the execution of these tasks until the <code>resume_tasks()</code> is called.</p> Pausing and Resuming Tasks<pre><code>await scheduler.pause_tasks(task_ids=[\"task-1\", \"task-2\"])\nawait scheduler.resume_tasks(task_ids=[\"task-1\", \"task-2\"])\n</code></pre>"},{"location":"schedulers/#updating-tasks","title":"Updating Tasks","text":"<p>Finally, you can update a Tasks <code>run_at</code> value by calling the <code>update_execution_times()</code> and passing in an <code>Iterable</code> of <code>TaskExecutionTime</code> objects.</p> Updating a Task's Execution Time<pre><code>from taskshed.models import TaskExecutionTime\n\nawait scheduler.update_execution_times(\n    tasks=[\n        TaskExecutionTime(task_id=\"task-1\", run_at=datetime.now() + timedelta(seconds=20)),\n        TaskExecutionTime(task_id=\"task-2\", run_at=datetime.now() + timedelta(seconds=30)),\n    ]\n)\n</code></pre>"},{"location":"workers/","title":"Workers","text":"<p>Workers are responsible for pulling due tasks from a datastore and executing their associated callbacks. TaskShed ships with two workers:</p> <ul> <li>EventDrivenWorker: Dynamically adjusts wakeups based on scheduled tasks. Lowest latency and best for single-process or tightly-coordinated systems where a scheduler can notify the worker of schedule changes.</li> <li>PollingWorker: Polls the datastore at a fixed interval for due tasks. Easier to run in fully distributed setups where the worker and scheduler are decoupled.</li> </ul> Task Latency <p>Task latency = (when the task actually ran) - (the task's scheduled run time).</p> <p>EventDrivenWorker generally achieves the lowest latency because it sets timers to wake the event loop exactly when tasks should run. PollingWorker has higher latency determined by the polling interval.</p> <p>Use EventDrivenWorker when you can run a Scheduler and a Worker in the same process. Use PollingWorker when you need to decouple the Scheduler and Worker, or run them on different machines.</p>"},{"location":"workers/#callback-map","title":"Callback Map","text":"<p>Every worker accepts a <code>callback_map</code> argument. This is a mapping of callback names (strings) to async callables (coroutines).</p> <p>The worker looks up <code>task.callback</code> in <code>callback_map</code>. If the callback name is not present the worker raises <code>IncorrectCallbackNameError</code>.</p> <p>Callback functions must be coroutines. If you need to use a synchronous function, wrap it in an <code>async</code> wrapper that offloads work to a thread or process.</p>"},{"location":"workers/#event-driven-worker","title":"Event Driven Worker","text":"<p>The EventDrivenWorker achieves the lowest possible task execution latency by waking up exactly when the next task is due, rather than polling on a fixed interval. This makes it ideal when precise timing is important and you want to minimise the delay between a task\u2019s scheduled time and when it actually runs.</p> <p>However, it must be notified whenever tasks are added, removed, or rescheduled in the datastore. In TaskShed, the Scheduler takes care of these notifications, so an EventDrivenWorker is typically passed directly to the scheduler when it\u2019s created.</p> Passing EventDrivenWorker to the AsyncScheduler<pre><code>from taskshed.datastores import InMemoryDataStore\nfrom taskshed.schedulers import AsyncScheduler\nfrom taskshed.workers import EventDrivenWorker\n\ndatastore = InMemoryDataStore()\nworker = EventDrivenWorker(\n    callback_map={\"callback\": callback},\n    datastore=datastore,\n)\n\nscheduler = AsyncScheduler(datastore=datastore, worker=worker)\n</code></pre>"},{"location":"workers/#polling-worker","title":"Polling Worker","text":"<p>The PollingWorker checks the datastore at regular intervals to see if any tasks are due. You can control the frequency using the optional <code>polling_interval</code> parameter (default: 3 seconds). A shorter interval reduces latency but increases datastore load.</p> PollingWorker With a Polling Interval of 1 Second<pre><code>from taskshed.workers import PollingWorker\n\nPollingWorker(\n    callback_map={\"callback\": callback},\n    datastore=get_scheduler_datastore(),\n    polling_interval=timedelta(seconds=1)\n)\n</code></pre> <p>Because it doesn\u2019t rely on notifications, the PollingWorker can operate completely independently from the scheduler. This makes it well-suited for distributed systems, where workers and the scheduler run on separate machines but share the same datastore.</p> <p>You can also run multiple PollingWorkers against the same datastore, even on different machines. This allows you to scale out processing capacity as each worker will claim and execute different tasks without interfering with the others.</p>"},{"location":"workers/#distributed-example","title":"Distributed Example","text":"<p>Below, the scheduler runs on one machine, a shared Redis datastore on another and the worker on a third.</p> schedule_tasks.py<pre><code>from datetime import datetime, timedelta\n\nfrom taskshed.datastores import RedisConfig, RedisDataStore\nfrom taskshed.schedulers import AsyncScheduler\n\ndatastore = RedisDataStore(\n    RedisConfig(\n        host=\"redis-datastore.example.com\",\n        port=16379,\n        username=\"user\",\n        password=\"password\",\n        ssl=True,\n    )\n)\n\nscheduler = AsyncScheduler(datastore)\n\n\nasync def main():\n    await scheduler.start()\n    while True:\n        await scheduler.add_task(\n            callback=\"say_hello\",\n            run_at=datetime.now() + timedelta(seconds=3),\n        )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    loop = asyncio.new_event_loop()\n    loop.create_task(main())\n    loop.run_forever()\n</code></pre> execute_tasks.py<pre><code>from taskshed.datastores import RedisConfig, RedisDataStore\nfrom taskshed.workers import PollingWorker\n\n\nasync def say_hello():\n    print(\"Hello!\")\n\n\ndatastore = RedisDataStore(\n    RedisConfig(\n        host=\"redis-datastore.example.com\",\n        port=16379,\n        username=\"user\",\n        password=\"password\",\n        ssl=True,\n    )\n)\nworker = PollingWorker(\n    callback_map={\"say_hello\": say_hello},\n    datastore=datastore,\n)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    loop = asyncio.new_event_loop()\n    loop.create_task(worker.start())\n    loop.run_forever()\n</code></pre>"}]}